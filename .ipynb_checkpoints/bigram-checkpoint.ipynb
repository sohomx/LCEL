{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.0.321 in /Users/sohom/cuda/lib/python3.11/site-packages (0.0.321)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain==0.0.321) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain==0.0.321) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain==0.0.321) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain==0.0.321) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain==0.0.321) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain==0.0.321) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain==0.0.321) (0.0.54)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain==0.0.321) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain==0.0.321) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain==0.0.321) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from langchain==0.0.321) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.321) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.321) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.321) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.321) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.321) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.321) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sohom/cuda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.321) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/sohom/cuda/lib/python3.11/site-packages (from anyio<4.0->langchain==0.0.321) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from anyio<4.0->langchain==0.0.321) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.321) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.321) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sohom/cuda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.321) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain==0.0.321) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain==0.0.321) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain==0.0.321) (4.7.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.321) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sohom/cuda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.321) (2023.7.22)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.321) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.321) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain==0.0.321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe the chain with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell a joke about {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Tell a joke about {topic}'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.5, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't programmers like nature? \n",
      "\n",
      "Because they prefer the indoors - there are too many bugs outside!"
     ]
    }
   ],
   "source": [
    "# streaming responses\n",
    "for s in chain.stream({\"topic\": \"programming\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# langchain components and the runnable protocol\n",
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ChatOpenAIInput',\n",
       " 'anyOf': [{'type': 'string'},\n",
       "  {'$ref': '#/definitions/StringPromptValue'},\n",
       "  {'$ref': '#/definitions/ChatPromptValueConcrete'},\n",
       "  {'type': 'array',\n",
       "   'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "     {'$ref': '#/definitions/HumanMessage'},\n",
       "     {'$ref': '#/definitions/ChatMessage'},\n",
       "     {'$ref': '#/definitions/SystemMessage'},\n",
       "     {'$ref': '#/definitions/FunctionMessage'}]}}],\n",
       " 'definitions': {'StringPromptValue': {'title': 'StringPromptValue',\n",
       "   'description': 'String prompt value.',\n",
       "   'type': 'object',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'StringPromptValue',\n",
       "     'enum': ['StringPromptValue'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['text']},\n",
       "  'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'A Message from an AI.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content', 'type': 'string'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'A Message from a human.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content', 'type': 'string'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content', 'type': 'string'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content', 'type': 'string'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'A Message for passing the result of executing a function back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content', 'type': 'string'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'}},\n",
       "   'required': ['content', 'name']},\n",
       "  'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',\n",
       "   'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n",
       "   'type': 'object',\n",
       "   'properties': {'messages': {'title': 'Messages',\n",
       "     'type': 'array',\n",
       "     'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "       {'$ref': '#/definitions/HumanMessage'},\n",
       "       {'$ref': '#/definitions/ChatMessage'},\n",
       "       {'$ref': '#/definitions/SystemMessage'},\n",
       "       {'$ref': '#/definitions/FunctionMessage'}]}},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ChatPromptValueConcrete',\n",
       "     'enum': ['ChatPromptValueConcrete'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['messages']}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ChatOpenAIOutput',\n",
       " 'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "  {'$ref': '#/definitions/HumanMessage'},\n",
       "  {'$ref': '#/definitions/ChatMessage'},\n",
       "  {'$ref': '#/definitions/SystemMessage'},\n",
       "  {'$ref': '#/definitions/FunctionMessage'}],\n",
       " 'definitions': {'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'A Message from an AI.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content', 'type': 'string'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'A Message from a human.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content', 'type': 'string'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content', 'type': 'string'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content', 'type': 'string'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'A Message for passing the result of executing a function back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content', 'type': 'string'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'}},\n",
       "   'required': ['content', 'name']}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the football team go to the bakery?\\n\\nBecause they needed a good roll!')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"Football\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Why did the AI go on a diet?\\n\\nBecause it had too many bytes!'),\n",
       " AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\")]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"AI\"}, {\"topic\": \"Science\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.batch([{\"topic\": \"AI\"}, {\"topic\": \"Science\"}], config={\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't satellites ever get married?\n",
      "\n",
      "Because they are always orbiting around!"
     ]
    }
   ],
   "source": [
    "async for s in chain.astream({\"topic\": \"Satellites\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Why don't AI robots ever get married?\\n\\nBecause they have trust issues with their algorithms!\"),\n",
       " AIMessage(content=\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.abatch([{\"topic\": \"AI\"}, {\"topic\": \"Science\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async stream with intermediate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.7.4-cp311-cp311-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Collecting tiktoken\n",
      "  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/fb/2a/3d02ef030f387c373acbeca6d5a2307405a1da735285ec12a9ed0b6302ea/tiktoken-0.5.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached tiktoken-0.5.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Obtaining dependency information for regex>=2022.1.18 from https://files.pythonhosted.org/packages/4d/d3/38b09813a32618acd437906c4d0194119e27139dbcd7486e69d58e375a27/regex-2023.10.3-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached regex-2023.10.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/sohom/cuda/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sohom/cuda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sohom/cuda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sohom/cuda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sohom/cuda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Using cached tiktoken-0.5.1-cp311-cp311-macosx_11_0_arm64.whl (924 kB)\n",
      "Using cached regex-2023.10.3-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "Installing collected packages: faiss-cpu, regex, tiktoken\n",
      "Successfully installed faiss-cpu-1.7.4 regex-2023.10.3 tiktoken-0.5.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Please answer the question based only on the following context:\n",
    "{context} \n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts([\"Ronaldo is the best football player in the world.\"], embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    {\"context\": retriever.with_config(run_name=\"Docs\"), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableBinding(bound=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x117f07850>), kwargs={}, config={'run_name': 'Docs'}),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Tell a joke about {topic}'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.5, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async for chunk in retrieval_chain.astream_log(\"who is the best football player in the world?\", include_names=[\"Docs\"]):\n",
    "#     print(\"-\"*40)\n",
    "#     print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = ChatPromptTemplate.from_template(\"Tell a joke about {topic}\") | model\n",
    "chain2 = ChatPromptTemplate.from_template(\"Write a short (2 lines) poem about {topic}\") | model\n",
    "combined = RunnableParallel(joke=chain1, poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  joke: ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Tell a joke about {topic}'))])\n",
       "        | ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.5, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy=''),\n",
       "  poem: ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Write a short (2 lines) poem about {topic}'))])\n",
       "        | ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.5, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')\n",
       "}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.8 ms, sys: 23.3 ms, total: 94.1 ms\n",
      "Wall time: 4.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Why did the computer break up with its partner?\\n\\nBecause they had too many \"hard drives\" and not enough \"RAM\"ance!'),\n",
       " AIMessage(content='Why did the couple go to the bakery?\\n\\nBecause they kneaded some dough-mance!')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain1.batch([{\"topic\": \"Love\"}, {\"topic\": \"Romance\"}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Love, a dance of souls intertwined,\\nForever bound, hearts aligned.'),\n",
       " AIMessage(content=\"Hearts entwined, love's sweet dance,\\nRomance blooms, a passionate trance.\")]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.batch([{\"topic\": \"Love\"}, {\"topic\": \"Romance\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-q7m0hH1OKLGDKrXUiaTG0r3T on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 143 ms, sys: 22.2 ms, total: 165 ms\n",
      "Wall time: 34.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'joke': AIMessage(content='Why did the two lovebirds go to therapy?\\n\\nBecause they had tweet-ment issues!'),\n",
       "  'poem': AIMessage(content='Love, a gentle flame that warms the soul,\\nBinding hearts together, making us whole.')},\n",
       " {'joke': AIMessage(content='Why did the romance novel go to therapy?\\n\\nBecause it had too many unresolved plot lines!'),\n",
       "  'poem': AIMessage(content=\"Love's gentle touch,\\nHearts entwined, forever much.\")}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combined.batch([{\"topic\": \"Love\"}, {\"topic\": \"Romance\"}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
